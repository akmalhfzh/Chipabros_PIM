import os
import torch
import torchvision.models as models
from transformers import BertModel, GPT2Model

NUM_OPERATIONS = 3000
os.makedirs("sim_cases", exist_ok=True)

def generate_hex(tensor_weights, target_sparsity, filename):
    flat_weights = torch.abs(tensor_weights.flatten())
    k_index = int(target_sparsity * flat_weights.numel())
    if k_index >= flat_weights.numel(): k_index = flat_weights.numel() - 1
    
    threshold = torch.kthvalue(flat_weights, k_index).values.item()
    mask = (flat_weights <= threshold).int().tolist()
    
    if len(mask) > NUM_OPERATIONS:
        start_idx = len(mask) // 2
        trace = mask[start_idx : start_idx + NUM_OPERATIONS]
    else:
        trace = (mask * (NUM_OPERATIONS // len(mask) + 1))[:NUM_OPERATIONS]
        
    with open(filename, "w") as f:
        for m in trace: f.write(f"{m}\n")
    print(f"✅ {filename} generated!")

resnet = models.resnet50(pretrained=True)
generate_hex(resnet.layer4[0].conv1.weight.data, 0.50, "sim_cases/meta_ResNet_50.hex")

bert = BertModel.from_pretrained("bert-base-uncased")
generate_hex(bert.encoder.layer[0].attention.self.query.weight.data, 0.75, "sim_cases/meta_BERT_NLP.hex")

gpt = GPT2Model.from_pretrained("gpt2")
generate_hex(gpt.h[0].mlp.c_fc.weight.data, 0.85, "sim_cases/meta_LLaMA3_8B.hex")
generate_hex(gpt.h[0].mlp.c_fc.weight.data, 0.90, "sim_cases/meta_GPT4_Sim.hex")

with open("sim_cases/meta_Baseline.hex", "w") as f:
    for _ in range(NUM_OPERATIONS): f.write("0\n")
print("✅ Baseline generated!")

# Zip filenya biar gampang didownload
!zip -r sim_cases.zip sim_cases/
